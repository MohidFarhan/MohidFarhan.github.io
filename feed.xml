<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-09-12T22:19:32+05:00</updated><id>/feed.xml</id><title type="html">Mohid Farhan | Exploring the Physics of the Universe</title><subtitle>A physics blog exploring quantum mechanics, relativity, and the universe&apos;s hidden structure. Created by Mohid Farhan.</subtitle><author><name>Mohid Farhan</name></author><entry><title type="html">Measuring the Cosmos: From Parallax to the Edge of the Universe</title><link href="/cosmology/astronomy/particle%20physics/astrophysics/Measuring-the-universe/" rel="alternate" type="text/html" title="Measuring the Cosmos: From Parallax to the Edge of the Universe" /><published>2025-08-30T00:00:00+05:00</published><updated>2025-08-30T00:00:00+05:00</updated><id>/cosmology/astronomy/particle%20physics/astrophysics/Measuring-the-universe</id><content type="html" xml:base="/cosmology/astronomy/particle%20physics/astrophysics/Measuring-the-universe/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Have you ever come across an enormous number with the words “light years” next to it? If yes, then naturally, one must wonder how we got to this number.
Are these distances even real, or do scientists make them up and pull them out of nowhere? I mean, who will question them anyway? Let’s answer these questions.</p>

<p>Today, we will take a deep dive into the ingenious methodology of determining cosmological distances.
We will start with methods of determining nearby objects and then progressively move to the farther objects, until we reach the edge of our universe.
To develop an understanding of astrophysics and cosmology, it is pivotal to understand the methods of determining large distances, so bear with me.</p>

<h2 id="the-stellar-parallax">The Stellar Parallax</h2>

<p>I want you to hold your thumb inches away from your eyes. Close your right eye while keeping the left one open and vice versa. Notice something?
You may have found that your thumb shifts position relative to the background. This is not because your thumb magically moved when you switched eyes, but has more to do with angles and how your brain perceives depth. By measuring the angle of the shift and knowing the distance between your eyes, you can determine how far away your thumb is. And all you need is basic trigonometry.
Why did I make you do this random task? That’s because scientists also do the same random task to determine stellar distances, but on a much larger scale.
Unfortunately, your eyes are too close for this method to work on distant stars, since the “shift” angle is too small and impossible to measure. Then how can we use this method?</p>

<p>The answer is surprisingly simple. You capture the star on your telescope. Then wait six months before doing the same again, having your telescope in the same location for maximum accuracy.
The idea is to leverage the distance traveled by the Earth, in order to attain a large enough angle such that it is measurable. After 6 months, the Earth is on the other side of the Sun, implying that the “distance between our eyes” is 2 Astronomical Units (AU) in-between the stellar snapshots, where AU is the distance between the Earth and the Sun. This makes sense in the diagram shown below.</p>

<p>Consider a nearby star observed from Earth at two opposite points in its orbit around the Sun, six months apart. The baseline of this observation is the diameter of Earth’s orbit, which is 2 AU, giving a right triangle with a baseline of 1 AU, distance to the star (d), and Parallax angle (p), measured in arcseconds which is one of 3600 parts of a degree.</p>

<p><img src="/assets/images/ParallaxDiagram_ManimCE_v0.19.0.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: A visualization of the trigonometry involved with stellar parallax (very very not to scale).</p>

<p>Using basic trigonometry:</p>

\[\tan p = \frac{1}{d}\]

<p>For very small angles (which parallax angles always are), we can use the small-angle approximation:</p>

\[\tan p \approx p \quad \text{(in radians)}\]

<p>Thus:</p>

\[p~(\text{radians}) = \frac{1~\text{AU}}{d}\]

<p>To convert to arcseconds:</p>

\[p~(\text{arcseconds}) = \frac{206265~\text{arcsec}}{\text{radian}} \cdot \frac{1~\text{AU}}{d}\]

<p>Defining 1 parsec as the distance at which a star has a parallax of 1 arcsecond:</p>

\[d~(\text{parsecs}) = \frac{1}{p~(\text{arcsec})}\]

<p>where 1 pc = 206265 AU.</p>

<p>This proves to be a simple yet elegant method of determining stellar distances. By measuring the shift angle in arcseconds, we can immediately obtain a reasonable estimate of its distance from us.</p>

<p>Stellar parallax yields very accurate results for determining distances of solar system objects, and other nearby neighboring stars, but breaks down beyond 10 Kpc due to our limitations of measuring angles. So, if you would have noticed, there exist background stars in the diagram, which do not shift much, if at all. These stars are beyond 10 Kpc. We, therefore, run into a challenge. How would we measure the distances to those stars?</p>

<h2 id="how-far-did-stellar-parallax-get-us">How far did Stellar Parallax get us?</h2>

<p>10 Kpc is an impressive distance, and already it’s an unfathomable distance. Upon conversion, we find that it is roughly 32,600 light years. This means that light, the fastest entity in the universe, takes 32,600 years to travel this distance!</p>

<p>If we see a star 30,000 LY from us, we are seeing the star as it was 30,000 years ago, because light that left it 30,000 years ago is only reaching us at that point. Conversely, if aliens inhabiting near the stars pointed their telescopes to us, assuming a powerful enough zoom, they would see humans painting mammoths on cave walls and walking on glaciers resulting from the latest ice age.</p>

<p>This is already an insane distance BUT our galaxy, the Milky Way, is 100,000 LY in diameter. We can barely determine the distances of one-third of the contents of our own galaxy, let alone those of other galaxies. This realization humbled us quickly and made us push to find another method that was successful at the larger scales.</p>

<h2 id="spectroscopic-parallax---a-misleading-name">Spectroscopic Parallax - A misleading name</h2>

<p>Stellar parallax can reach distances up to approximately 10,000 parsecs, but only with extremely precise instruments like the Gaia space telescope, and only for relatively bright stars. For dimmer or more distant stars, parallax becomes unreliable. In such cases, spectroscopic parallax offers an alternative method.</p>

<p>This method does not involve actual parallax angles. The star’s spectral type and luminosity class are determined using its spectrum. From that classification we know the intrinsic brightness of the star and an estimate of the star’s absolute magnitude (M) is obtained using standard stellar models or HR diagrams. m is the apparent magnitude of a star if it were 10 pc from Earth.</p>

<p>The apparent magnitude (M) is a measure of the star’s brightness as seen from Earth, hence the word “apparent”. This quantity can be measured observationally through a telescope. This quantity can be small despite the star being very bright, given that it’s distant. This is because of the fact that brightness falls off with distance.
M and m is all you need to determine the distance, thanks to the distance modulus formula.</p>

\[m - M = 5 \log_{10}(d) - 5\]

<p>Solving for d:</p>

\[d = 10^{\frac{m - M + 5}{5}} \quad \text{(in parsecs)}\]

<p>The derivation of this formula, as well as how it can be used, is in the appendix. For now, all you need to know is that the formula leverages the fact that the larger the deficit in the apparent and absolute magnitude of a star, the greater the distance. If the apparent magnitude is equal to the absolute magnitude, the distance is 10 pc by definition. If the apparent magnitude is much smaller than the absolute magnitude, then the star appears dim although it is intrinsically bright, meaning that it must be much farther than 10 pc, and vice versa, if the star appears bright but is actually dim, it must be much closer than 10 pc.
This formula determines the exact values, and will also be useful ahead.</p>

<h2 id="cepheid-variable-stars">Cepheid Variable Stars</h2>

<p>The method of spectroscopic parallax only improved our readings for dim stars, without improving much of our range. To go intergalactic, we turn to Cepheid variable stars. Cepheid variable stars are pulsating stars whose brightness varies in a predictable cycle. Their importance comes from a precise relationship between their pulsation period and intrinsic luminosity.</p>

<p>Discovered by Henrietta Swan Leavitt while studying stars in the Small Magellanic Cloud. All Cepheids in the same galaxy are roughly at the same distance, allowing the variation in brightness to be attributed to intrinsic luminosity. The longer the pulsation period, the brighter the star.</p>

<p>This relationship allows astronomers to infer the absolute magnitude M from the observed period. The empirical Period–Luminosity relation for Classical Cepheids is often written as:</p>

\[M = a \log_{10}(P) + b\]

<p>where M is the absolute magnitude, P is the period in days, and a and b are calibration constants determined observationally.</p>

<p><img src="/assets/images/B3P2.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: The Magnitude plotted as a function of the period of the star’s brightness cycle.</p>

<p>Once M is known, the task is the same. We determine m observationally and use the distance modulus formula again. By leveraging the best of today’s technology, the James Webb Space Telescope, this method gives us readings of up to 20 Mpc. This translates to 60 million LY which is so insane, that if aliens that far away were to point their telescopes on Earth, they would see the event that wiped out the dinosaurs, or even the dinosaurs if they’re lucky!.
This upgrade easily increases our range from a fraction of our own galaxy to, comfortably, the local group and the Virgo supercluster.</p>

<h2 id="type-1a-supernovae">Type 1a Supernovae</h2>

<p>60 Mpc is not bad but in cosmological scale, even calling it the tip of the iceberg would be generous. Let’s keep going farther and farther. In the regime we are currently in, the distance modulus remains our only hope and so the challenge is to keep on coming up with methods of determining the absolute magnitude and the intrinsic brightness of the distant objects. I write “objects” and not stars because stars are not bright enough anymore, but supernovae sure are.
Supernovae are violent explosions that occur due to the gravitational collapse of huge stars. They are known to be among the most energetic events in the known universe. You know it’s serious when a single event outshines galaxies for weeks.</p>

<p>Type Ia supernovae are thermonuclear explosions of white dwarfs in binary systems. When the white dwarf accretes enough mass from its companion star to approach the Chandrasekhar limit (about 1.4 solar masses), it undergoes runaway nuclear fusion and explodes. These supernovae have very consistent intrinsic luminosities, making them excellent standard candles.</p>

<p>These supernovae have very consistent intrinsic luminosities, making them excellent standard candles. Their peak absolute magnitude is approximately around 19.3. The light curve (brightness vs time) follows a predictable shape, which can be used to refine the intrinsic brightness even further. These supernovae were instrumental in determining that the universe’s expansion was accelerating, leading to the concept of Dark Energy, which will be a discussion for a future blog.
Once again, since we know M, we can easily know everything else needed to use the distance modulus formula.</p>

<p>But by using the Type 1a supernovae as standard candles, our distance determination range increases to a whopping 1 Gpc, by using the James Webb Space Telescope. This is the equivalent to 3 billion LY.
This upgrade means that the aliens would now see an Earth restricted only to unicellular, microbial life. This younger, 1.5-billion year old Earth was a lot hotter than today, with completely different continents, and a whole lot of volcanic activity and crater formations. It is believed that Mars had liquid water at around this time, though that is debatable.</p>

<h2 id="cosmological-redshift">Cosmological Redshift</h2>

<p>3 billion LY is almost a quarter of the observable universe. There’s more than 3 quarters STILL left, which makes us realize the utter scale of the universe we are a part of. There are no events bright enough for us to reliably determine M and latch onto the distance modulus formula, so is this it?
The answer is a resounding no thanks to the final boss: cosmological redshift. In this method, astronomers rely on the expansion of space itself to determine distances using redshift. As light travels through expanding space, its wavelength stretches. This effect is called redshift, denoted by z.</p>

<p>The farther away a galaxy is, the more its light is redshifted. This relationship was first discovered by Edwin Hubble in 1929. For small redshifts where z exceeds or is near 0.1, the velocity at which a galaxy appears to be receding is:</p>

\[v = H d\]

<p>where v is the recession velocity, H is the Hubble constant (units: km/s/Mpc), and d is the distance to the object in megaparsecs (Mpc).</p>

<p>Solving for distance:</p>

\[d = \frac{v}{H} = \frac{cz}{H}\]

<p>c is the speed of light. This approximation works well for nearby galaxies (100 MLY). For more distant objects where z exceeds 1, cosmological models must be used to relate redshift and distance accurately.
At high redshifts, the universe’s expansion has changed over time, so the simple linear Hubble law is replaced by integrals over the expansion history. Redshift-based methods can measure distances to quasars and galaxies billions of light-years away.</p>

<p>Combined with supernova observations, they have revealed that the universe’s expansion is accelerating. While the cosmological modeling is a complex topic that will be covered in future blogs, the surreal fact is that with the high z methods, we have been able to track the distance of objects that are almost as old as the universe, which is estimated to be around 13.8 billion years old.</p>

<p>Viewing an object this far away, you’re seeing so far back in time, that if you could see only a few thousand years more back in time, you would see the big bang. As a matter of fact, this is of primary interest to cosmologists who are motivated to learn about the big bang and early universe. This method of viewing back to the ancient universe is a cornerstone of observational High Energy Physics and, outside of large detectors and colliders, remains our best beacon of hope to understand the origin of the universe.</p>

<p>As for our alien friends looking in our direction, they would not see us and might never see us and we might never see them. Due to the expansion of the universe, we would recede from each other faster than the speed of light. Meaning that the photons trying to reach us are fighting for a lost cause. The ruthless expansion of the universe means that, despite being the fastest entity in the universe, their finish line is moving away from them faster than they can run. And this will continue to happen till the end of time.</p>

<p>And as always, keep on physicsing.</p>

<h1 id="appendix">Appendix</h1>

<h2 id="deriving-the-distance-modulus">Deriving the distance modulus</h2>

<p>Astronomers use the <strong>magnitude</strong> scale, where a star appears <em>fainter</em> if its magnitude is <em>larger</em>. This comes from two facts:</p>

<ol>
  <li><strong>Magnitude–flux relation:</strong></li>
</ol>

\[m_1 - m_2 = -2.5 \log_{10}\left(\frac{F_1}{F_2}\right)\]

<ol>
  <li><strong>Inverse-square law for brightness:</strong></li>
</ol>

\[F(d) = \frac{L}{4\pi d^2}\]

<p>where L is the luminosity and d is the distance.</p>

<hr />

<p>Now, let m be the apparent magnitude of a source at distance d, and M its absolute magnitude (i.e. what it would look like at 10 pc). Using the flux ratio:</p>

\[m - M = -2.5 \log_{10}\left(\frac{F(d)}{F(10\,\text{pc})}\right)\]

<p>Substitute the inverse-square law:</p>

\[m - M = -2.5 \log_{10}\left(\frac{L/(4\pi d^2)}{L/(4\pi (10\,\text{pc})^2)}\right)\]

<p>Simplify:</p>

\[m - M = -2.5 \log_{10}\left(\frac{(10\,\text{pc})^2}{d^2}\right)\]

\[m - M = -5 \log_{10}\left(\frac{10}{d}\right)\]

<p>Rearrange:</p>

\[m - M = 5 \log_{10}\left(\frac{d}{10}\right)\]

<p>And finally, writing explicitly with d in parsecs:</p>

\[m - M = 5 \log_{10}(d) - 5\]

<p>Thus, as d increases, the flux falls and m rises, making the star <strong>fainter in appearance but larger in magnitude</strong>.</p>

<hr />

<h2 id="example">Example</h2>

<p><strong>Problem:</strong> A star has apparent magnitude m = 10 and absolute magnitude M = 5. Find its distance.</p>

<p>Start with the distance modulus:</p>

\[m - M = 5 \log_{10}(d) - 5\]

<p>Insert values:</p>

\[10 - 5 = 5 \log_{10}(d) - 5\]

\[5 + 5 = 5 \log_{10}(d)\]

\[10 = 5 \log_{10}(d)\]

\[\log_{10}(d) = 2\]

\[d = 10^2 = 100 \ \text{pc}\]

<hr />

<p><strong>Sanity check:</strong> At twice the distance (200 pc), flux drops by a factor of 4. Magnitude increases by 1.5. The star would look fainter with an apparent magnitude of 11.5, consistent with the magnitude definition.</p>]]></content><author><name>Mohid Farhan</name></author><category term="Cosmology" /><category term="Astronomy" /><category term="Particle Physics" /><category term="Astrophysics" /><summary type="html"><![CDATA[How do we measure something we can’t touch? From the tiny wiggle of a nearby star against the night sky to the stretching fabric of space itself, astronomers have built a cosmic ruler that spans billions of light-years. Each step extends our reach deeper into the universe. We will dive into how we use a simple combination of eyepieces, lenses and gratings to peek into the edge of the universe.]]></summary></entry><entry><title type="html">Inside the Atom: Where ‘Empty Space’ Teems with Activity</title><link href="/quantum%20field%20theory/relativistic%20quantum%20mechanics/particle%20physics/atomic%20and%20molecular%20physics/inside-the-atom-where-empty-space-teems-with-activity/" rel="alternate" type="text/html" title="Inside the Atom: Where ‘Empty Space’ Teems with Activity" /><published>2025-08-16T00:00:00+05:00</published><updated>2025-08-16T00:00:00+05:00</updated><id>/quantum%20field%20theory/relativistic%20quantum%20mechanics/particle%20physics/atomic%20and%20molecular%20physics/inside-the-atom-where-empty-space-teems-with-activity</id><content type="html" xml:base="/quantum%20field%20theory/relativistic%20quantum%20mechanics/particle%20physics/atomic%20and%20molecular%20physics/inside-the-atom-where-empty-space-teems-with-activity/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Today we will unravel the idea that in physics, there is no such thing as empty space. What we like to call “empty” is teeming with activity and fluctuations at the quantum scales. This is not just theory, but has measurable consequences, all of which we will get into as we go forward, so enjoy the adventure. You will need a grasp of basic quantum mechanics, electromagnetic theory and a bit of atomic and molecular physics to go with that.</p>

<p>Our understanding of the atom was a hot topic for physicists in the early 1900s, with J.J. Thompson’s famous “fruitcake” model, where the atom was assumed to be a smooth and filled sphere of positive charge, with clumps of negative charges embedded in it.  In 1911, Ernest Rutherford devised the most accurate atomic model known to man, backed by an experiment, where a minority of alpha particles were observed to scatter off at large angles when targeted at a thin foil of gold. This showed that an atom was mostly empty space with a lot of its mass being concentrated at the center, leading to such dramatic scattering. This center with positive charge is known as a nucleus, with negative electrons orbiting around it.</p>

<h2 id="the-empty-space-in-an-atom">The Empty Space in an Atom</h2>

<p>Now that we objectively know that most of the atom is empty space, lets make estimations to quantify this emptiness. The Rutherford model predicts the radius of the atom to be of the order \(r_a \approx 10^{-10}m\) and of the nucleus to be \(r_n \approx 10^{-15}m\). Comparing the ratios of their volumes gives:</p>

\[\frac{\frac{4 \pi (r_a)^3}{3}}{\frac{4 \pi (r_n)^3}{3}}=\frac{r_a^3}{r_n^3} \approx (\frac{10^{-10}}{10^{-15}})^3 \approx 10^{15}\]

<p>This tells us that the atom is almost completely empty. To  give you an idea of the magnitude of this ratio, its like if the atom was the size of a football field, the nucleus would be a cherry placed in its center. Now, let’s dive into the deep end. If I claimed that there is no such thing as empty space, that what is actually going on in that space?</p>

<p>To understand this, we will view the atom from the lens of Quantum Field Theory (QFT).</p>

<h2 id="the-behaviour-of-the-electrons">The Behaviour of the Electrons</h2>

<p>In the classical treatment of the atom, the electrons should spiral into the nucleus while radiating energy and ultimately the atom should collapse in picoseconds assuming that it is purely a particle. However, in the quantum mechanical treatment, it is assumed to be a wave, in the sense that all of its information is inscribed in its wavefunction \(\psi\), which can be computed by solving the Schrodinger equation:</p>

\[\left[-\frac{\hbar^2}{2m} \nabla^2 - \frac{e^2}{4\pi \varepsilon_0 r} \right] \psi = E \psi\]

<p>This \(\psi\) acts like a probability that is distributed throughout empty space, which do not locate the electron exactly, but rather give the probability of where we are likely to find it. As it turns out, the resulting \(\psi\) leads to discrete energy levels which keep the atom stable, and the electron’s energy constant, with different energies corresponding to different shells. While all this points to the fact that empty space is not truly empty due to a probability being smeared all across it, the real game-changer is the addition of special relativity and relativistic quantum mechanics.</p>

<h2 id="particles-in-a-contracting-box">Particles in a Contracting Box</h2>

<p>One can intuitively picture the atom as a small sphere, which contrains the particles confined within it. To understand the dominance of quantum effects, let’s analyze the example of a particle in a contracting box.  The Heisenberg Uncertainty Principle says that the smaller the uncertainty in position, the larger the uncertainty in momentum and vice versa.</p>

\[\Delta p \Delta x \geq \frac{\hbar}{2}\]

<p>So if an electron is constrained to a small enough space, relativistic effects become dominant. In such a case, the characteristic energy can be approximated as:</p>

\[E \approx \Delta pc \approx mc^2\]

\[\frac{\hbar c}{2\Delta x}=mc^2\]

\[\Delta x=\frac{\hbar}{2m_ec} \approx 10^{-13}m\]

<p>So this means that if I were to constrain anything within \(10^{-13}m\), which is the case in an atom, the correct characteristic energy can match a critical value of 1.02 MeV, which is the energy required to produce the positron-electron pair, setting the stage for jitters within the atom.</p>

<!-- Animation of vacuum fluctuations -->
<p align="center">
  <img src="/assets/images/Adobe Express - AtomicVacuumFluctuations.gif" width="500" alt="Vacuum fluctuations inside the atom" />
  <br /><em>Simulation 1: Vacuum fluctuations appearing and disappearing within the “empty” region of the atom</em>
</p>

<p>It becomes apparent that \(10^{-13}m\) is bigger than the radius of the nucleus but smaller than that of the atom, hence, making the “empty” space inside the atom an ideal environment for effects to occur, as shown in the figure. The nucleus is not drawn to scale; the scale that was mentioned earlier (Cherry to a football field). It is now easy to visualize that the idea of a constant number of particles breaks down, and therefore the pure Quantum Mechanical picture becomes blurry. To incorporate and account for the creation and annhilation of particles, we need a deeper theory which treat the number of particles as variables, unlike quantum mechanics. This is where Quantum Field Theory (QFT) shines. The name of this theory is actually very self explanatory. In the QFT, quantized fields are fundamental, not particles. Particles are just localized excitations within that field. The perpective shift from pure particles (classical mechanics), to waves (quantum mechanics) to, now, fields (QFT) proves fruitful in explaining the whole picture as I will show you shortly.</p>

<h2 id="the-picture-without-qft">The Picture Without QFT</h2>

<p>Before diving head-first into QFT, Let’s pick a benchmark first, and treat it without QFT. We pick the simplest case here: the ground-state Hydrogen atom, with an electron orbiting a proton. We can estimate the ground-state energy of the hydrogen atom using the Heisenberg uncertainty principle.</p>

<p>The total energy of the electron is approximately the sum of its kinetic and potential energies:</p>

\[E \approx K + U\]

<p>I say approximately here since this is the non-relativistic treatment and has it’s shortcomings as we discussed in the previous blog. Here the kinetic energy \(K\) is given by</p>

\[K = \frac{(\Delta p)^2}{2 m_e}\]

<p>and the potential energy \(U\) is the Coulomb potential:</p>

\[U = - \frac{e^2}{4 \pi \varepsilon_0 \Delta x}.\]

<p>Here, \(\Delta x\) is the characteristic size of the electron’s orbit (or its position uncertainty), and \(\Delta p\) is the momentum uncertainty.</p>

<p>From the Heisenberg uncertainty principle:</p>

\[\Delta x \, \Delta p \gtrsim \frac{\hbar}{2} \quad \implies \quad \Delta p = \frac{\hbar}{\Delta x}.\]

<p>Substituting \(\Delta p\) into the kinetic energy gives:</p>

\[K = \frac{\hbar^2}{2 m_e (\Delta x)^2}.\]

<p>Thus, the total energy as a function of \(\Delta x\) is:</p>

\[E(\Delta x) = \frac{\hbar^2}{2 m_e (\Delta x)^2} - \frac{e^2}{4 \pi \varepsilon_0 \Delta x}.\]

<p>To find the equilibrium (minimum) energy, differentiate with respect to \(\Delta x\) and set it to zero. For the untrained reader, this might seem like an operation that came out of nowhere, and let’s try to make it make sense. All particles (or waves or fields depending upon which side you are on) in the universe have the same mission: to achieve stability. Every single particle like to lose energy and settle comfortably to the lowest energy possible and not an ounce more than that. In this case, that particle so happens to be the electron, and it is no different. It wants to orbit the nucleus such that its energy is minimized. The best way to that is to plot the energy as a function of \(\Delta x\) and look for the value of \(\Delta x\) where the energy is at its minimum. This would be the distnce from the nucleus that the electron would be most comfortable in being and, therefore, you are most likely to find the electron at this distance.</p>

<p><img src="/assets/images/HAGS.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: The energy as a function of distance for the hydrogen atom.</p>

\[\frac{dE}{d (\Delta x)} = - \frac{\hbar^2}{m_e (\Delta x)^3} + \frac{e^2}{4 \pi \varepsilon_0 (\Delta x)^2} = 0.\]

<p>Solving for \(\Delta x\):</p>

\[\frac{\hbar^2}{m_e (\Delta x)^3} = \frac{e^2}{4 \pi \varepsilon_0 (\Delta x)^2} 
\quad \implies \quad 
\Delta x = \frac{4 \pi \varepsilon_0 \hbar^2}{m_e e^2}.\]

<p>This reproduces the famous Bohr Radius, so thats a great sign that our working is not only correct, but also makes perfect sense. Finally, substituting this value of \(\Delta x\) back into the total energy expression:</p>

\[E_1 = - \frac{m_e e^4}{2 (4 \pi \varepsilon_0)^2 \hbar^2} \approx -13.6~\mathrm{eV}.\]

<p>This reproduces the exact ground-state energy of the hydrogen atom.</p>

<p>For the confined particle, if we take the assumption that \(\psi\) vanishes at the edges, the allowed wavelengths are given by:</p>

\[\lambda_n=\frac{2L}{n}\]

<!-- Animation of the Allowed Wavefunctions -->
<p align="center">
  <img src="/assets/images/Adobe Express - PIAB.gif" width="500" alt="Vacuum fluctuations inside the atom" />
  <br /><em>Simulation 2: A dynamic visualization of the allowed wavefunctions that disappear at boundaries resulting in modes</em>
</p>

<p>The edges (where probability disappears) are called nodes, which have distinct spatial and temporal frequencies. n is an integer value and L is the length of the constraining box. The wavenumber k (spatial frequency) is given by \(k=\frac{2 \pi}{\lambda}\), which results in:</p>

\[k=\frac{\pi n}{L}\]

<p>Expanding into 3 dimensions, we get the spatial frequency vector:</p>

\[k=\frac{\pi}{L}(n_x,n_y,n_z)\]

<p>The temporal frequency, on the other hand, is given by:</p>

\[\omega_k=2 \pi f=\frac{2 \pi c}{\lambda}=kc\]

<h2 id="the-qft-treatment">The QFT Treatment</h2>

<p>Quantum Field Theory pictures a field as infinite coupled modes, each acting as harmonic oscillators.</p>

<!-- Animation of the QFT Interpretation -->
<p align="center">
  <img src="/assets/images/Adobe Express - QFTPlaneCoupled (1).gif" width="500" alt="Vacuum fluctuations inside the atom" />
  <br /><em>Simulation 3: A visualization of a quantized fields comprised of coupled modes acting as harmonic oscillators</em>
</p>

<p>One can think of simulation 3 above as a 3D extension of simulation 2. The minimum energy of a quantum harmonic oscillator is, unlike classical oscillators, non zero due to the Heisenberg Uncertainty Principle. Why am I blaming HUP for this? Well, picture this…</p>

<p>A classical oscillator (like a pendulum) can sit at zero kinteic energy. As a matter of fact, that is exactly what happens if you don’t keep on applying a force to it since it loses energy to the environment. BUT, why can’t a quantum harmonic oscillator do the same? Thats because if its stationary, we know its position with certainty, and its momentum with certainty (zero). And since, HUP must be obeyed by the particles at the quantum scale, this can not happen. So when, \(\Delta x\) becomes small, the uncertainties in momentum are a source of maintaining a minimum energy, and vice vera. Hopefully, that makes sense intuitively. Now, let’s have it make sense mathematically.</p>

<p>Consider a one-dimensional quantum harmonic oscillator with mass \(m\) and angular frequency \(\omega\). Its Hamiltonian is:</p>

\[\hat{H} = \frac{\hat{p}^2}{2m} + \frac{1}{2} m \omega^2 \hat{x}^2,\]

<p>where \(\hat{p} = -i\hbar \frac{d}{dx}\) is the momentum operator, and the potential term (second term on the right hand side) arises from \(E=\frac{1}{2} k x^2\) where spring constant k takes the form \(k=m \omega^2\)</p>

<p>To find the ground-state energy, we look for the state that minimizes the total energy. Using the Heisenberg uncertainty principle:</p>

\[\Delta x \, \Delta p \ge \frac{\hbar}{2},\]

<p>we estimate the kinetic and potential contributions in terms of \(\Delta x\):</p>

\[E \approx \frac{(\Delta p)^2}{2m} + \frac{1}{2} m \omega^2 (\Delta x)^2
= \frac{\hbar^2}{8 m (\Delta x)^2} + \frac{1}{2} m \omega^2 (\Delta x)^2.\]

<p><img src="/assets/images/QHOGS.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: The energy as a function of distance for the quantum harmonic oscillator.</p>

<p>This gives the total energy as a function of \(\Delta x\). To find the minimum, differentiate with respect to \(\Delta x\) and set the derivative to zero:</p>

\[\frac{dE}{d(\Delta x)} = -\frac{\hbar^2}{4 m (\Delta x)^3} + m \omega^2 (\Delta x) = 0.\]

<p>Solving for \(\Delta x\):</p>

\[(\Delta x)^4 = \frac{\hbar^2}{4 m^2 \omega^2} \implies \Delta x = \sqrt{\frac{\hbar}{2 m \omega}}.\]

<p>Substituting back into the energy expression:</p>

\[E_{\min} = \frac{\hbar^2}{8 m (\Delta x)^2} + \frac{1}{2} m \omega^2 (\Delta x)^2
= \frac{\hbar^2}{8 m} \cdot \frac{2 m \omega}{\hbar} + \frac{1}{2} m \omega^2 \cdot \frac{\hbar}{2 m \omega} = \frac{1}{2} \hbar \omega.\]

<p>Thus, the ground-state energy of the quantum harmonic oscillator is</p>

\[E_0 = \frac{1}{2} \hbar \omega.\]

<p>This non-zero energy is called the zero-point energy, reflecting the fact that even in its lowest-energy state, the oscillator still exhibits quantum fluctuations. This seemingly innocent characteristic has a profound consequences since it gives rise to vacuum fluctuations. Now, the picture start to become clear from a theoretical point of view.</p>

<p>So we can see that by incorporating QFT, these modes that make up the vacuum give rise to vacuum energy, due to quantum fluctuations. This bold prediction is made possible by the QFT treatment. Analyzing the effect due to one mode, we combine the previous 2 equations to get:</p>

\[E_{min}=\frac{\hbar\omega_k}{2}=\frac{\hbar ck}{2}\]

<p>According to Maxwell equations of Electromagnetism, the energy density of a mode k is given by:</p>

\[u=\frac{\epsilon_0|\epsilon_k|^2}{2}\]

<p>\(\epsilon_k\) is the root mean square amplitude of the electric field associated with k. Note that the magnetic field is not mentioned since its weaker than the electron field by a factor of c. The energy is simply the product of energy density and volume. Assuming our confining box has length a, we get:</p>

\[E_{vac}=ua^3=\frac{\epsilon_0|\epsilon_k|^2a^3}{2}\]

<p>We can, now, equate \(E_{vac}\) with \(E_{min}\) in a semi classical approximation to get:</p>

\[\frac{\epsilon_0|\epsilon_k|^2a^3}{2}=\frac{\hbar ck}{2}\]

<p>Though this is a semi-classical approximation, ultimately it leads to the same result. The resulting root mean square of the electric field amplitude of a mode is:</p>

\[|\epsilon_k|^2=\frac{\hbar ck}{\epsilon_0 a^3}\]

<p>So the vacuum electric field amplitude associated with mode k is given by:</p>

\[\epsilon_k=\sqrt{\frac{\hbar ck}{\epsilon_0 a^3}}\]

<p>We now observe, from the QFT perspective,  that every mode trembles with its own electric field. Note that in QFT, there are infinite amount of modes and each one contributes its own effects, in a vacuum, oblivious to the presence or absence of particles. Now the idea of empty space not being empty starts to make sense, but we are not finished. Each mode within the Hydrogen atom, influences the electron, not by much alone, but the effects add up. This also changes the probability distribution of the electron, and is something we can pick up despite its subtlety.</p>

<h2 id="the-lamb-shift">The Lamb Shift</h2>

<p>This subtle change in the electron’s probability distribution leads to a shift in its energy levels, in what is referred to as “The Lamb Shift”. If you bear with me a little longer, we can get into how we can quantify it for our Hydrogen atom. Let’s first study the shift caused by one mode, with spatial wave-vector k and frequency \(\omega_k\), on an electron. The force experienced by the electron due to an electric field is given by:</p>

\[F=m_ea=e\epsilon\]

<p>So the acceleration caused by a mode k is given by:</p>

\[a_k=\frac{e\epsilon_k}{m_e}\]

<p>The time period of the oscillation of mode k is given by:</p>

\[T_k=\frac{2\pi}{\omega_k}\]

<p>The time where the field pushes the electron in one direction is half of this time period (picture a pendulum):</p>

\[\tau=\frac{\pi}{\omega_k}=\frac{\pi}{ck}\]

<p>The displacement (\(s_k\)) caused due to a mode is given by:</p>

\[s_k=\frac{a_k\tau^2}{2}\]

\[s_k=\frac{e\epsilon_k\pi^2}{2m_ec^2k^2}=\frac{e\pi^2}{2m_ec^2k^2}\sqrt{\frac{\hbar ck}{\epsilon_0 a^3}}=\frac{e\pi^2}{2m_e c^2 k^{3/2}} \left( \frac{\hbar c}{\varepsilon_0 a^3} \right)^{1/2}\]

<p>Now we can calculate the summed up effects on displacement, due to all the modes. Since these modes are not correlated with each other, we can sum the mean square displacement. The summation takes the following form over k-space:</p>

\[s^2=\sum_k (s_k^2)\]

<p>Taking the continuum limit over k-space gives us:</p>

\[s^2=\sum_k (s_k^2)=\frac{a^3}{(2\pi)^3}\int_k(s_k)^2d^3k\]

<p>The factor \(\frac{a^3}{(2\pi)^3}\) arises due to the density of states in k-space, which factors in for the step-size of k and allows the sum to take the form of an integral. Since we are dealing with a spherical atom, it is useful to use spherical coordinates where:</p>

\[d^3k=4\pi k^2dk\]

<p>We can now substitute in \(d^3k\) and \(s_k\) to give us an integral in terms of k.</p>

\[s^2=\frac{a^3}{(2\pi)^3}\int_k \left( \frac{e\pi^2}{2m_e c^2 k^{3/2}} \left( \frac{\hbar c}{\varepsilon_0 a^3} \right)^{1/2} \right)^2 4\pi k^2dk\]

<p>Simplifying gives:</p>

\[s^2=\frac{\pi^2\hbar e^2}{8m^2\epsilon_0c^3}\int_k\frac{1}{k}dk\]

<p>If we are not careful, and integrate from zero to infinity, we could get unphysical answers. Therefore, we are going to integrate over the cut-off limits, since not all modes can interact with the electron. We introduce the IR-UV (Infrared-UltraViolet) cutoffs, since modes smaller than the IR modes are have wavelengths too high to perturb the electron and modes greater than the UV modes, have wavelengths that are too small compared to the atoms, which means that the idea of small perturbations on the electron does no longer apply. We define the cutoffs as:</p>

\[k_{UV}=\frac{1}{\lambda_c}, \quad k_{IR}=\frac{1}{a_0}\]

<p>where \(\lambda_c=\frac{\hbar}{mc}\) is the Compton wave length which we derived earlier and \(a_0\) is the Bohr radius.</p>

<p>Which gives us the integral:</p>

\[s^2=\frac{\pi^2\hbar e^2}{8m^2\epsilon_0c^3}\int_{k_{UV}}^{k_{IR}}\frac{1}{k}dk\]

<p>Performing the integration yields:</p>

\[s^2=\frac{\pi^2\hbar e^2}{8m^2\epsilon_0c^3}In\left(\frac{1}{\alpha}\right)\]

<p>where, remarkably, \(\alpha\) is the fine structure constant and is of the form:</p>

\[\alpha=\frac{e^2}{4\pi \epsilon_0\hbar c}\approx\frac{1}{137}\]

<p>which is the strength of the electromagnetic interaction. This constant is baked into nature and thecfact that it pops up here also, is a very encouraging sign that we are on the right track. This is the beauty of physics, where the mathematics magically leads you to the most fundamental constants in nature. The average displacement registered by the electron can be computationally derived through simulations, the details of which we will not get into in this blog, and it turns out to be:</p>

\[\bar{s}=\frac{s^2}{3a_0}\]

<p>To determine the energy shift on the Hydrogen atom, we begin by considering the potential:</p>

\[V(r)=\frac{-e^2}{4\pi \epsilon_0r}\]

<p>Factoring in the perturbation of the electron yields, by the Taylor Expansion:</p>

\[V(r+s)=V(r)+s\frac{dV}{dr}+...\]

\[V(r+s)=V(r)+s\frac{e^2}{4\pi \epsilon_0r^2}+...\]

<p>The second term on the left hand side is the energy due to the perturbation, and is precisely what we are looking for. So letting \(r=a_0\) (the most probable value of r for the ground-state Hydrogen atom) gives us:</p>

\[\Delta E=s\frac{e^2}{4\pi \epsilon_0a_0^2}\]

\[\Delta E=\frac{s^2}{3a_0}\frac{e^2}{4\pi \epsilon_0a_0^2}=\frac{e^2s^2}{12\pi \epsilon_0a_0^3}\]

<p>Using the \(s^2\) relation derived earlier gives:</p>

\[\Delta E=\frac{e^2}{12\pi \epsilon_0a_0^3}\frac{\pi^2\hbar e^2}{8m^2\epsilon_0c^3}In\left(\frac{1}{\alpha}\right)\approx5 \times10^{-5} eV\]

<p>Note that every single quantity in this expression is a known constant, and so plucking in all the constants gives you a reasonable order-of-magnitude value the change in energy due to empty space not being actually empty. Although this energy is a million times smaller than the ground-state energy of Hydrogen which was stated earlier, it is a measurable change which acts as undeniable and irrefutable proof that the vacuum is far from empty: it always jitters and its fingerprints are real and measurable. So we just proved by both theory and experiment, that even empty space when quantized to a small enough scale, gives rise to fluctuations. If you constrain something to a small enough volume, the Heisenberg Uncertainty Principle allows it to acquire enough energy to produce particle-anti particle pairs.</p>

<p>I hope you learned something new from today’s blog, and I will be back soon with another interesting topic which is now, almost a century long mystery.</p>]]></content><author><name>Mohid Farhan</name></author><category term="Quantum Field Theory" /><category term="Relativistic Quantum Mechanics" /><category term="Particle Physics" /><category term="Atomic and Molecular Physics" /><summary type="html"><![CDATA[A subtle shift in hydrogen’s energy levels cracked open the door to quantum field theory which revealed the dynamic, buzzing vacuum inside every atom.]]></summary></entry><entry><title type="html">Limitations of the Schrödinger Equation and the Birth of Particle Physics</title><link href="/cosmology/relativistic%20quantum%20mechanics/particle%20physics/relativity/limitations-of-schrodinger/" rel="alternate" type="text/html" title="Limitations of the Schrödinger Equation and the Birth of Particle Physics" /><published>2025-08-02T00:00:00+05:00</published><updated>2025-08-02T00:00:00+05:00</updated><id>/cosmology/relativistic%20quantum%20mechanics/particle%20physics/relativity/limitations-of-schrodinger</id><content type="html" xml:base="/cosmology/relativistic%20quantum%20mechanics/particle%20physics/relativity/limitations-of-schrodinger/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Welcome to this blog series where the objective is to spread science and make difficult concepts digestible. The content of this blog is for an audience familiar with basic Quantum Mechanics and concepts of matrices. I will show you that with such basic tools, one can track all the way back to the founding equation of particle physics which governs our universe. However, be advised, this blog series is not for the faint-hearted. You must take on the math</p>

<p>The well-known Schrödinger equation is given by:</p>

<p>\(i\hbar \frac{\partial}{\partial t} \psi(\mathbf{r}, t) = \left[ -\frac{\hbar^2}{2m} \nabla^2 + V(\mathbf{r}, t) \right] \psi(\mathbf{r}, t)\)  ———–(1)</p>

<p>This equation is a cornerstone of Quantum Mechanics. It provides accurate predictions for many atomic and molecular systems. In essence, it is an energy conservation equation which states the total energy operator  of a particle is the sum of its kinetic energy operator and potential energy operator. The first term on the left hand side arises from the time-dependent Schrödinger equation which captures the evolution of the state. It is a first-order partial differential equation, which means that once the wavefunction \( \psi(\mathbf{r}, t) \) is known, we can determine the evolution of the particle , with time-independent potential, using:</p>

<p>\(\psi(\mathbf{r}, t) = \psi(\mathbf{r}, 0) e^{-\frac{iHt}{\hbar}}\)  ———–(2)</p>

<p>The probability density associated with the wavefunction is given by:</p>

<p>\(\rho(\mathbf{r}, t) = |\psi(\mathbf{r}, t)|^2\)  ———–(3)</p>

<h2 id="the-continuity-equation-for-the-schrödinger-equation">The Continuity Equation for the Schrödinger Equation</h2>

<p>Let’s do a simple analysis of this equation using the continuity equation. If you are familiar with electromagnetic theory, this equation governs how the movement of charge density (\( \rho \)) influences the divergence of current density (j). In EMT, this equation is responsible for the conservation of charge.</p>

\[\frac{d}{dt}\int\rho(\mathbf{r},t)d^3x=-\int j(\mathbf{r},t)dS\]

<p>Using the Gauss Divergence theorem:</p>

\[\frac{d}{dt}\int\rho(\mathbf{r},t)d^3x=-\int \nabla \cdot j(\mathbf{r},t)\, d^3x\]

<p>Integrating over finite or closed volume yields the continuity equation:</p>

\[\frac{d\rho}{dt}+\nabla \cdot j=0\]

<p>Remarkably, the same treatment can be applied to the Schrödinger equation and conserve another crucial quantity. We begin with the time-dependent Schrödinger equation:</p>

<p>\(i\hbar \frac{\partial \psi}{\partial t} = \left(-\frac{\hbar^2}{2m} \nabla^2 + V \right)\psi\)  ———–(4)</p>

<p>And taking the complex conjugate of both sides, we get:</p>

<p>\(-i\hbar \frac{\partial \psi^\star}{\partial t} = \left(-\frac{\hbar^2}{2m} \nabla^2 + V \right)\psi^\star\)  ———–(5)</p>

<p>Now multiply Eq. (4) by \( \psi^\star \) and Eq. (5) by \( \psi \):</p>

<p>\(\psi^\star i\hbar \frac{\partial \psi}{\partial t} = \psi^\star \left( -\frac{\hbar^2}{2m} \nabla^2 \psi + V\psi \right)\)  ———–(6)</p>

<p>\(\psi (-i\hbar) \frac{\partial \psi^\star}{\partial t} = \psi \left( -\frac{\hbar^2}{2m} \nabla^2 \psi^\star + V\psi^\star \right)\)  ——–(7)</p>

<p>Subtracting the two equations:</p>

<p>\(i\hbar \left( \psi^\star \frac{\partial \psi}{\partial t} + \psi \frac{\partial \psi^\star}{\partial t} \right) = -\frac{\hbar^2}{2m} \left( \psi^\star \nabla^2 \psi - \psi \nabla^2 \psi^\star \right)\)  ———–(8)</p>

<p>Note that the terms containing the potential V cancel out. We now take \(i\hbar\) on the other side, and subtract both terms:</p>

<p>\(\frac{\partial}{\partial t} (\psi^\star \psi) - \nabla \cdot \left[ \frac{-i\hbar}{2m} \left( \psi^\star \nabla \psi - \psi \nabla \psi^\star \right) \right] = 0\) ———-(8b)</p>

<p>Recognizing the first term on the left-hand side as the time derivative of the probability density:</p>

<p>\(\frac{\partial}{\partial t} (\psi^\star \psi) = \frac{\partial \rho}{\partial t}\)   ———–(9)</p>

<p>The second left-hand side term becomes the divergence of current density j:</p>

<p>\(\nabla \cdot \left( \frac{\hbar}{2mi} (\psi^\star \nabla \psi - \psi \nabla \psi^\star) \right)\)   ———–(10)</p>

<p>Substituting (9) and (10) into (8b), we obtain the continuity equation:</p>

<p>\(\frac{\partial \rho}{\partial t} + \nabla \cdot \mathbf{j} = 0\)   ———–(11)</p>

<p>Where:</p>

<p>\(\rho = \psi^\star \psi\)   ———–(12)</p>

<p>\(\mathbf{j} = \frac{\hbar}{2mi} (\psi^\star \nabla \psi - \psi \nabla \psi^\star)\)   ———–(13)</p>

<p>In this case, we find that the equation that was used to conserve charge in a seemingly unrelated context of EMT also applies to Quantum Mechanics to conserve probability. An important takeaway here is the fact that probability density is always positive. This is the beauty of physics, where all different scenarios are fundamentally governed by conservation mechanisms. Such mechanisms will be extensively discussed in future blogs.</p>

<p>For the Schrodinger equation, the positive probability density is a physically viable result since we expect probability to not be negative. But as the title suggests, this equation has its shortcomings. While it describes the dynamics accurately at low velocities, it is inherently non-relativistic. This means that it does not do a great job when applied to particles moving close to the speed of light. This stems from the fact that it arises from a classical expression. Furthermore, there is no mention of anti-particles, and spin needs to be added “by hand”. Keeping these limitations in mind, let’s try a relativistic treatment of this equation.</p>

<h2 id="the-klein-gordon-equation">The Klein-Gordon Equation</h2>

<p>We begin with the relativistic energy-momentum relation:</p>

<p>\(E^2 = p^2c^2 + m^2c^4\)   ———–(14)</p>

<p>Now we promote energy and momentum to operators in quantum mechanics:</p>

<p>\(E \rightarrow i\hbar \frac{\partial}{\partial t}, \quad \mathbf{p} \rightarrow -i\hbar \nabla\)   ———–(15)</p>

<p>Applying the operators in (15) and a wavefunction \(\psi\) onto (14), we get:</p>

<p>\((i\hbar \frac{\partial}{\partial t})^2 \psi = \left[ (-i\hbar \nabla)^2 c^2 + m^2 c^4 \right] \psi\)  </p>

<p>This simplifies to:</p>

<p>\(-\hbar^2 \frac{\partial^2 \psi}{\partial t^2} = \left[ -\hbar^2 c^2 \nabla^2 + m^2 c^4 \right] \psi\)  </p>

<p>Rearranging terms gives:</p>

<p>\(\frac{\partial^2 \psi}{\partial t^2} - c^2 \nabla^2 \psi + \left( \frac{m^2 c^4}{\hbar^2} \right) \psi = 0\) ——–(16) </p>

<p>This is the Klein-Gordon equation in its expanded form.</p>

<p>We can now write it in a more compact and relativistically manifest form:</p>

<p>\(\left( \frac{1}{c^2} \frac{\partial^2}{\partial t^2} - \nabla^2 + \frac{m^2 c^2}{\hbar^2} \right) \psi = 0\)  </p>

<p>This equation, developed by Oskar Klein and Walter Gordon, is Lorentz invariant and thus compatible with special relativity. However, in doing so, we have introduced other glaring issues. First of all, it is second-order in time, meaning that the wavefunction \(\psi(\mathbf{r}, t)\) alone is not sufficient to determine the system’s evolution. This violates a fundamental postulate of quantum mechanics, which states that the dynamics of a system can be determined if its wavefunction is known. But here, especially for a relativistically relevant case, we require complete information about the wavefunction and its first order derivative to determine dynamics. Also, this equation does not mention spin or predict antiparticles, painting an incomplete picture. Among its flaws is the failure to accurately reproduce the energy levels of hydrogen and the fine-structure constant. It might, at this point, appear that I have personal issues with this equation though I assure you that is not the case. And unfortunately its problems do not stop here either, as deriving the continuity equation from the Klein-Gordon equation reveals another critical discrepancy.</p>

<h2 id="continuity-equation-for-klein-gordon">Continuity Equation for Klein-Gordon</h2>
<p>The procedure that was applied on the Schrodinger equation will be repeated here but all important steps will be displayed so its easier to follow. The Klein-Gordon equation is:</p>

\[\left( \frac{1}{c^2} \frac{\partial^2}{\partial t^2} - \nabla^2 + \frac{m^2 c^2}{\hbar^2} \right) \psi = 0\]

<p>Its complex conjugate is:</p>

\[\left( \frac{1}{c^2} \frac{\partial^2}{\partial t^2} - \nabla^2 + \frac{m^2 c^2}{\hbar^2} \right) \psi^\star = 0\]

<p>Now multiply the former equation with \(\psi^\star\) and the latter with \(\psi\), and subtract the results:</p>

<p>\(\psi^\star \frac{\partial^2 \psi}{\partial t^2} - \psi \frac{\partial^2 \psi^\star}{\partial t^2} - c^2 \left( \psi^\star \nabla^2 \psi - \psi \nabla^2 \psi^\star \right) = 0\)  </p>

<p>Using the product rule identities, this becomes:</p>

<p>\(\frac{\partial}{\partial t} \left( \psi^\star \frac{\partial \psi}{\partial t} - \psi \frac{\partial \psi^\star}{\partial t} \right) + c^2 \nabla \cdot \left( \psi \nabla \psi^\star - \psi^\star \nabla \psi \right) = 0\)  </p>

<p>To make the dimensions consistent with the continuity equation, we define the probability density and current as:</p>

<p>\(\rho = \frac{i\hbar}{2mc^2} \left( \psi^\star \frac{\partial \psi}{\partial t} - \psi \frac{\partial \psi^\star}{\partial t} \right)\)  </p>

<p>\(\mathbf{j} = \frac{i\hbar}{2m} \left( \psi \nabla \psi^\star - \psi^\star \nabla \psi \right)\)  </p>

<p>We now obtain the continuity equation in the usual form:</p>

<p>\(\frac{\partial \rho}{\partial t} + \nabla \cdot \mathbf{j} = 0\)  </p>

<p>This looks familiar — but there’s a problem. The expression for \(\rho\) yields a negative if the second term dominates the first. That is:</p>

<p>\(\rho &lt; 0\) if 
\(|\psi \frac{\partial \psi^\star}{\partial t}| &gt; |\psi^\star \frac{\partial \psi}{\partial t}|\)  </p>

<p>This is the case when a plane-wave solution of \(\psi\) is applied. Note that this is an oversimplification, since the deep rooted cause of negative probability density were the negative energy solutions. Since both the causes lead to the same result, i.e. negative probabilities, I stuck to the simpler solution. One can see from (14) that negative energy solutions are not only allowed but necessary, and this equation fails to make sense of such solutions. Moreover, negative probability desnity violates the probabilistic interpretation of quantum mechanics, because probability density must always be non-negative. So, in trying to make our equation relativistically correct, we have introduced a fatal flaw: the Klein-Gordon equation allows negative probabilities. And not to mention all the other red-flags that we associated with this equation but a negative probability density is considered blasphemy in quantum mechanics.</p>

<h2 id="what-now">What now?</h2>

<h2 id="diracs-ingenious-insight">Dirac’s Ingenious Insight</h2>
<p>Paul Dirac, a brilliant British physicist, tackled the problems of the Klein-Gordon equation by proposing a linear form of the energy equation:</p>

\[E = (\alpha_x p_x + \alpha_y p_y + \alpha_z p_z)c + \beta mc^2\]

<p>Now, squaring both sides:</p>

\[E^2 = \left((\alpha_x p_x + \alpha_y p_y + \alpha_z p_z)c + \beta mc^2\right)^2\]

<p>Expanding the square:</p>

\[\begin{aligned}
E^2 &amp;= \alpha_x^2 p_x^2 c^2 + \alpha_y^2 p_y^2 c^2 + \alpha_z^2 p_z^2 c^2 + \beta^2 m^2 c^4 \\
&amp;\quad + (\alpha_x \alpha_y + \alpha_y \alpha_x) p_x p_y c^2 + (\alpha_x \alpha_z + \alpha_z \alpha_x) p_x p_z c^2 \\
&amp;\quad + (\alpha_y \alpha_z + \alpha_z \alpha_y) p_y p_z c^2 + (\alpha_x \beta + \beta \alpha_x) p_x m c^3 \\
&amp;\quad + (\alpha_y \beta + \beta \alpha_y) p_y m c^3 + (\alpha_z \beta + \beta \alpha_z) p_z m c^3
\end{aligned}\]

<p>In order for this expression to reduce to the relativistic energy relation \(E^2 = p^2 c^2 + m^2 c^4\), a number of constraints must be satisfied, as the last 6 terms must vanish.
Dirac imposed the following constraints:</p>

\[\alpha_x^2 = \alpha_y^2 = \alpha_z^2 = 1\]

\[\beta^2 = 1\]

\[\alpha_i \alpha_j + \alpha_j \alpha_i = 0\]

\[\alpha_i \beta + \beta \alpha_i = 0\]

<p>These can be summarized compactly:</p>

\[\alpha_i \alpha_j + \alpha_j \alpha_i = 0 \quad \text{(17)}\]

\[\alpha_i \beta + \beta \alpha_i = 0 \quad \text{(18)}\]

\[\alpha_i^2 = 1 \quad \text{(19)}\]

\[\beta^2 = 1 \quad \text{(20)}\]

<p>There began the race to find a quantity that would satisfy (17)-(20) simultaneously.</p>

<h3 id="why-scalars-wont-work">Why Scalars Won’t Work</h3>

<p>Assume \(\alpha\) is a scalar. Then (17) implies:</p>

\[\alpha_i \alpha_j = -\alpha_j \alpha_i\]

<p>This can only be true if either \(\alpha_i = 0\) or \(\alpha_j = 0\), which contradicts (19). So \(\alpha\) cannot be a scalar.</p>

<h3 id="the-pauli-matrix-attempt">The Pauli Matrix Attempt</h3>

<p>This was where Dirac’s genius had begun to shine. Normally, one would conclude this to be a dead route since no scalar number satisfies all constraints at once. The Pauli matrices were then tried. Although Dirac claimed to have derived these himself, it is not consensus as these matrices are usually associated with Wolfgang Pauli, another prominent physicist of that time.
A famous property of these matrices is that they satisfy:</p>

<p>\(\sigma_i \sigma_j + \sigma_j \sigma_i = 0\) 
for \(i \ne j\)
and 
\(\sigma_i^2 = \mathbb{I}\)</p>

<p>So (17) and (19) are satisfied.
We begin with the Pauli matrices:</p>

\[\sigma_x = \begin{bmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{bmatrix}, \quad
\sigma_y = \begin{bmatrix}
0 &amp; -i \\
i &amp; 0
\end{bmatrix}, \quad
\sigma_z = \begin{bmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{bmatrix}\]

<p>Assume a general \(2 \times 2\) matrix for \(\beta\):</p>

\[\beta = \begin{bmatrix}
a &amp; b \\
c &amp; d
\end{bmatrix}\]

<p>Applying the anti-commutation relation with \(\sigma_x\):</p>

\[\sigma_x \beta + \beta \sigma_x =
\begin{bmatrix}
c + b &amp; d + a \\
a + d &amp; b + c
\end{bmatrix} = 0\]

<p>This implies:</p>

\[c = -b, \quad d = -a\]

<p>So \(\beta\) becomes:</p>

\[\beta = \begin{bmatrix}
a &amp; b \\
-b &amp; -a
\end{bmatrix}\]

<p>Now test this against \(\sigma_y\):</p>

\[\sigma_y \beta + \beta \sigma_y =
\begin{bmatrix}
2ib &amp; 0 \\
0 &amp; 2ib
\end{bmatrix} = 0 \Rightarrow b = 0\]

<p>Thus:</p>

\[\beta = \begin{bmatrix}
a &amp; 0 \\
0 &amp; -a
\end{bmatrix}\]

<p>Now test this \(\beta\) against \(\sigma_z\):</p>

\[\sigma_z \beta + \beta \sigma_z =
\begin{bmatrix}
2a &amp; 0 \\
0 &amp; 2a
\end{bmatrix} = 0 \Rightarrow a = 0\]

<p>So finally:</p>

\[\beta = \begin{bmatrix}
0 &amp; 0 \\
0 &amp; 0
\end{bmatrix}\]

<p>This is the <strong>null matrix</strong>, which violates the constraint \(\beta^2 = 1\). Hence, <strong>no \(2 \times 2\) matrix can satisfy all of Dirac’s anticommutation relations</strong>. A higher-dimensional structure is necessary.</p>

<h2 id="44-matrix-solution-to-the-dirac-equation">4×4 Matrix Solution to the Dirac Equation</h2>

<p>So there is, once again, a violation of the constraints. It would be natural for someone to give up by now but not this guy. Matter of fact, he doubled down…Literally. He went one step further and imagined the matrices \(\alpha\) to be \(4 \times 4\) matrices, posed as block forms made of \(2 \times 2\) matrices.</p>

<p>The form is as follows:</p>

\[\alpha_x =\begin{pmatrix}
0 &amp; \sigma_x \\
\sigma_x &amp; 0
\end{pmatrix}=
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}\]

\[\alpha_y = \begin{pmatrix}
0 &amp; \sigma_y \\
\sigma_y &amp; 0
\end{pmatrix}=\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; -i \\
0 &amp; 0 &amp; i &amp; 0 \\
0 &amp; -i &amp; 0 &amp; 0 \\
i &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}\]

\[\alpha_z = \begin{pmatrix}
0 &amp; \sigma_z \\
\sigma_z &amp; 0
\end{pmatrix} = \begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0
\end{bmatrix}\]

<p>The beauty of this formalism lies in the fact that all the previous constraints are still satisfied. The only difference is that instead of \(2 \times 2\) matrices of scalars, we now use \(4 \times 4\) matrices made from \(2 \times 2\) matrix blocks.
Let’s test the anticommutation relation between \(\alpha_x\) and \(\alpha_y\):</p>

\[\alpha_x \alpha_y + \alpha_y \alpha_x =
\begin{bmatrix}
\sigma_x \sigma_y + \sigma_y \sigma_x &amp; 0 \\
0 &amp; \sigma_x \sigma_y + \sigma_y \sigma_x
\end{bmatrix}=
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}\]

<p>Now for \(\alpha_x^2\):</p>

\[\alpha_x^2 =
\begin{bmatrix}
\sigma_x^2 &amp; 0 \\
0 &amp; \sigma_x^2
\end{bmatrix}=
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}\]

<p>The same applies to \(\alpha_y^2\) and \(\alpha_z^2\), confirming:</p>
<ul>
  <li>\(\alpha_i \alpha_j + \alpha_j \alpha_i = 0\) for \(i \ne j\)</li>
  <li>
\[\alpha_i^2 = \mathbb{I}\]
  </li>
</ul>

<p>Let \(\beta\) be:</p>

\[\beta =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1
\end{bmatrix}\]

<p>This satisfies:</p>

\[\beta^2 =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
= \mathbb{I}_{4 \times 4}\]

<p>Now check the anticommutation between \(\alpha_x\) and \(\beta\):</p>

\[\alpha_x \beta + \beta \alpha_x =
\begin{bmatrix}
0 &amp; \sigma_x \\
\sigma_x &amp; 0
\end{bmatrix}
\begin{bmatrix}
\mathbb{I} &amp; 0 \\
0 &amp; -\mathbb{I}
\end{bmatrix}
+
\begin{bmatrix}
\mathbb{I} &amp; 0 \\
0 &amp; -\mathbb{I}
\end{bmatrix}
\begin{bmatrix}
0 &amp; \sigma_x \\
\sigma_x &amp; 0
\end{bmatrix}=
\begin{bmatrix}
0 &amp; -\sigma_x \\
\sigma_x &amp; 0
\end{bmatrix}
+
\begin{bmatrix}
0 &amp; \sigma_x \\
-\sigma_x &amp; 0
\end{bmatrix}=
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}\]

<p>Let \(\beta\) be:</p>

\[\beta =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1
\end{bmatrix}\]

<p>This choice satisfies the condition:</p>

\[\beta^2 =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}\]

<p>Now check the anticommutation relation between \(\alpha_x\) and \(\beta\):</p>

\[\alpha_x \beta + \beta \alpha_x =
\begin{bmatrix}
0 &amp; \sigma_x \\
\sigma_x &amp; 0
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{bmatrix}
+
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; -1
\end{bmatrix}
\begin{bmatrix}
0 &amp; \sigma_x \\
\sigma_x &amp; 0
\end{bmatrix}=
\begin{bmatrix}
0 &amp; -\sigma_x \\
\sigma_x &amp; 0
\end{bmatrix}
+
\begin{bmatrix}
0 &amp; \sigma_x \\
-\sigma_x &amp; 0
\end{bmatrix}=
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}\]

<p>Thus, this construction satisfies all of Dirac’s constraints, confirming that \(4 \times 4\) matrices are the minimal representation necessary to maintain compatibility with both quantum mechanics and special relativity. This derivation was long, but it involved the most simple matrix algebra that I hope you were able to follow. You must note that other matrices of the same dimensions also satisfy all constraints but we followed Dirac’s exact journey.</p>

<h2 id="operator-form-of-the-dirac-equation">Operator Form of the Dirac Equation</h2>

<p>Now that we have attained equations that satisfy all constraints simultaneously, let’s go back to the earlier form of the energy equation. Energy was written as a linear combination of momentum. Now apply the momentum and energy operators to a wavefunction \(\psi\) so</p>

\[E \psi = (\alpha_x p_x + \alpha_y p_y + \alpha_z p_z)c \psi + \beta m c^2 \psi\]

<p>becomes:</p>

\[i\hbar \frac{\partial \psi}{\partial t} = -i\hbar c \left( \alpha_x \frac{\partial \psi}{\partial x} + \alpha_y \frac{\partial \psi}{\partial y} + \alpha_z \frac{\partial \psi}{\partial z} \right) + \beta m c^2 \psi\]

<hr />

<h2 id="covariant-form-of-the-dirac-equation">Covariant Form of the Dirac Equation</h2>

<p>We have obtained the time-dependent Dirac equation, but this form is not readily found in textbooks. To simplify, we assume <strong>natural units</strong> where \(\hbar = c = 1\).</p>

<p>Defining the gamma matrices as:</p>

\[\gamma^0 = \beta, \quad \gamma^i = \beta \alpha_i \quad (i = x, y, z)\]

<p>Also defining the spacetime 4-gradient (which is a fancy way of saying that we are account for all dimensions, the 3 of space and 1 of time):</p>

\[\partial_\mu = \left( \frac{1}{c} \frac{\partial}{\partial t}, -\nabla \right), \quad
\text{and } \gamma^\mu = (\gamma^0, \gamma^1, \gamma^2, \gamma^3)\]

<p>This allows us to write the Dirac equation more compactly:</p>

\[\left( i\hbar \sum_{\mu = 0}^3 \gamma^\mu \partial_\mu - m c \right) \psi = 0\]

<p>Using the <strong>Einstein summation convention</strong>, we drop the summation symbol:</p>

\[\boxed{(i\hbar \gamma^\mu \partial_\mu - m c)\psi = 0}\]

<p>which gives us the Dirac equation in its most minimal and elegant form. This equation solved everything as it was relativistically correct and therefore computes the energy levels of the Hydrogen atom better than the Schrodinger equation. It introduces no negative probability density anomalies and remarkably also predicts spin states and anti-particles. Since the derivative is a 4-vector so must be the wavefunction. Its four components allows for the elegant inclusion of both the spin states of, both, the particle and its anti-particle counterpart. .I can go on with details since this is just the beginning of particle physics, but let’s leave the Dirac wavefunction for another blog. This remains, to date, the most complete equation in all of quantum mechanics. Remarkably, the equation remains obscure to many due to the mathematical complexity involved. This elegant formalism allows interpretation of negative energy solutions as antiparticles, which were later explained by <strong>Richard Feynman</strong> as normal particles traveling <strong>back in time</strong>. BUT, the shortcomings never end…</p>

<h2 id="the-strong-force-problem">The Strong Force Problem</h2>

<p>The Dirac equation still cannot be reliably applied to the <strong>strong interaction</strong>, the binding force at the core of the atomic nucleus. This challenge lies at the heart of <strong>nuclear physics</strong>. The strength of the electromagnetic force (otherwise known as the electromagnetic coupling constant) is
\(\alpha_{\text{EM}} = \frac{e^2}{4\pi \epsilon_0 \hbar c} \approx \frac{1}{137}\)</p>

<p>One may also recognise this term as the fine-structure constant. The Dirac equation works beautifully when this is the dominant force due to its low couplings strength. However, the strong force is MUCH stronger. So strong, in fact, that at low energies, the strong force coupling becomes dangerously close to 1. This presents a major issue for <strong>perturbative methods</strong>, which expand physical quantities as power series in the coupling constant.</p>

<p>The general perturbative series looks like:</p>

\[A(\alpha) = \sum_{i=0}^n \alpha^i A^{(i)} = A^{(0)} + \alpha A^{(1)} + \alpha^2 A^{(2)} + \cdots + \alpha^n A^{(n)}\]

<p>When \(\alpha \geq 1\), the series diverges, rendering all of our known analytical treatments as useless. So the next time you are part of a nuclear physics lecture, know that every single potential and equation that involves the strong nuclear force is an approximation that fits experiments and has not be analytically derived. 
And who knows, maybe if you can figure out a way to solve problems the same way Dirac did, you might be able to figure this out. At that point, you would have a realistic shot of being nominated as a Nobel Prize laureate.</p>

<p>Thank you for making it this far, and as always, keep on physicsing.</p>]]></content><author><name>Mohid Farhan</name></author><category term="Cosmology" /><category term="Relativistic Quantum Mechanics" /><category term="Particle Physics" /><category term="Relativity" /><summary type="html"><![CDATA[Why the Schrödinger equation fails to describe relativistic particles, and how the Dirac equation saves the day.]]></summary></entry></feed>